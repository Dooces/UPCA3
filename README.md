â€œeverything is in W + microâ€‘edit + hard physics gatesâ€ 


UPCA â€” Unified Predictive Cyclic Architecture
Structuralâ€‘Physics Edition (Wâ€‘Only, Typed Specialisation)

One substrate, one physics, simple rules, emergent complexity.
UPCA is a single evolving operator 
W
W that is the worldâ€‘model. All structure â€” from millisecond delays to abstract semantic relationships â€” is encoded as vertices and edges inside 
W
W. There are no parallel tables, no shadow memories; timing, sequence, rhythm, and concepts all manifest as walks and modes of a single graph.

Every proposed change to 
W
W must prove itself on a fixed set of behavioural probes and pass strict global safety gates before itâ€™s accepted. This keeps growth mechanical and selfâ€‘justifying.

Specialisation is not done with separate modules, but emerges as typed subgraphs (semantic, temporal, phase, feature) and longâ€‘range â€œwiresâ€ inside the same operator. These types allow you to monitor and balance use without splitting the underlying model.

ğŸ§  Mental model
text

Events / probes -->  [    W (graph)    ]  --> predictions / responses
                          |   |   |
            temporal ----/    |    \---- semantic
             phase  -----------|--------------- features
                               |
                          safety gates
                        (edit-as-experiment)
Nodes: tokens, delay hops, higherâ€‘order contexts, phase anchors, semantic concepts/operators.
Edges: directed weights linking nodes; can be shortâ€‘term temporal, longâ€‘term semantic, or mediated via a â€œterritoryâ€ vertex.
Physics: one state update rule applies to all.
Probes: fixed impulses/chords that â€œringâ€ the network â€” some temporal, some semantic.
Edit loop: propose a small change, sandbox it, measure probe impacts, accept/reject, project back to stability.
ğŸš€ Quickstart conceptual recipe
Maintain W as a cappedâ€‘sparse + lowâ€‘rank adjacency over a registry of vertices.
Inject events as oneâ€‘hot or chord vectors 
u
t
u 
t
â€‹
  into the state:
s
t
+
1
=
Ï•
(
W
s
t
+
u
t
)
s 
t+1
â€‹
 =Ï•(Ws 
t
â€‹
 +u 
t
â€‹
 ).
Run probes at intervals to monitor temporal and semantic behaviours.
Propose tiny structural edits (
Î”
W
Î”W) â€” e.g., a few edges, or add a new delay/concept node.
Shadowâ€‘rollout all probes with 
W
+
Î”
W
W+Î”W:
Compute trajectory MSE vs. baseline.
Compute discrete KL on monitored nodes.
Check typeâ€‘specific improvements and interference tolerance.
Accept only if thresholds and budgets pass. Project and reâ€‘cap weights to spectral safety bound.
Repeat at a slow cadence to let structure consolidate.


UPCA â€” State of Design (W-Only, Structural-Physics Edition)
0) Purpose and stance
One substrate: a single evolving operator 
ğ‘Š
W is the world-model. No shadow tables, no side predictors.

Everything is a walk: timing, sequence, rhythm, semantics, and â€œmacrosâ€ are encoded as vertices/edges and manifest via impulse responses and modes.

One control surface: global gain 
ğ‘”
(
ğ‘¡
)
g(t), spectral safety projection, and edit-as-experiment gates. Nothing else.

Typed specialisation without split models: specialisation arises as typed subgraphs (mediated paths) and long-range wiring, not as separate modules.

1) Core state, update, and constraints
State: 
ğ‘ 
ğ‘¡
âˆˆ
ğ‘…
âˆ£
ğ‘‰
âˆ£
s 
t
â€‹
 âˆˆR 
âˆ£Vâˆ£
  (activity over vertices).

Update: 
ğ‘ 
ğ‘¡
+
1
=
ğœ™
(
ğ‘Š
â€‰
ğ‘ 
ğ‘¡
+
ğ‘¢
ğ‘¡
)
s 
t+1
â€‹
 =Ï•(Ws 
t
â€‹
 +u 
t
â€‹
 ), with 
ğœ™
=
Ï•= identity (default) or clipped ReLU if needed for numerical safety.

Input 
ğ‘¢
ğ‘¡
u 
t
â€‹
 : one-hot or small chords (probes/events).

Operator: 
ğ‘Š
=
ğ‘†
+
ğ‘ˆ
ğ‘‰
âŠ¤
W=S+UV 
âŠ¤
 ; 
ğ‘†
S row-capped sparse, 
ğ‘ˆ
,
ğ‘‰
âˆˆ
ğ‘…
âˆ£
ğ‘‰
âˆ£
Ã—
ğ‘Ÿ
U,VâˆˆR 
âˆ£Vâˆ£Ã—r
  low-rank (default 
ğ‘Ÿ
=
8
r=8).

Signs: off-diagonals 
â‰¥
0
â‰¥0. (Inhibition via row-normalization + diagonal damping.)

Row cap: 
â„“
1
â„“ 
1
â€‹
  per final row 
â‰¤
1
â‰¤1 after composing 
ğ‘†
+
ğ‘ˆ
ğ‘‰
âŠ¤
S+UV 
âŠ¤
 .

Global budgets: 
ğ‘˜
row
=
32
k 
row
â€‹
 =32 (split by type below). 
âˆ£
ğ¸
âˆ£
â‰¤
20
âˆ£
ğ‘‰
âˆ£
âˆ£Eâˆ£â‰¤20âˆ£Vâˆ£.

Spectral safety: spectral radius 
ğœŒ
(
ğ‘Š
)
â‰¤
ğœŒ
\*
=
0.98
Ï(W)â‰¤Ï 
\*
 =0.98. After accept, rescale 
ğ‘Š
â†
ğ‘Š
â‹…
ğœŒ
\*
ğœŒ
^
+
10
âˆ’
3
Wâ†Wâ‹… 
Ï
^
â€‹
 +10 
âˆ’3
 
Ï 
\*
 
â€‹
 .

Damping: add diagonal 
ğ·
=
ğ›¿
ğ¼
D=Î´I, 
ğ›¿
=
0.02
Î´=0.02.

2) Vertex set and typed paths (no extra matrices)
Token nodes: observed symbols/events.

Delay nodes: 
Î”
1
,
â€¦
,
Î”
ğ¾
Î” 
1
â€‹
 ,â€¦,Î” 
K
â€‹
  (default 
ğ¾
=
8
K=8); chain 
Î”
ğ‘˜
â€‰â£
â†’
â€‰â£
Î”
ğ‘˜
+
1
Î” 
k
â€‹
 â†’Î” 
k+1
â€‹
 ; token 
â†’
Î”
1
â†’Î” 
1
â€‹
 ; 
Î”
ğ‘˜
â€‰â£
â†’
Î” 
k
â€‹
 â†’token edges implement â€œemit after 
ğ‘˜
k ticks.â€

Context nodes: promoted bigram/trigram contexts 
ğ¶
[
â‹…
]
C[â‹…] (Phase-2 only; gated).

Phase nodes: 
Î˜
ğ‘š
Î˜ 
m
â€‹
  (2â€“3) ring motifs for lightweight periodic binding.

Semantic concept nodes: e.g., Fruit, Red, Company, Edible, Toolâ€¦ (budgeted pool).

Operator nodes (semantic transforms): e.g., Hypernym, Hyponym, Pluralize, ColorOf, PartOfâ€¦ (optional, minimal).

Mediator vertices (territories): TEMP, SEM, PHASE, FEAT.
A typed edge is the 2-hop path x â†’ MED(type) â†’ y. This enables territory-targeted probes/ablations without a second model.

3) Probes (readouts only)
Probe pool (size 256):

Temporal core (128): single-node impulses; impulse pairs; timing chords through 
Î”
Î” chain.

Semantic core (128): concept-chord probes (e.g., {Fruit}, {Fruit+Red}, {Tool+Cut}, {Company+Computer}), operator tests (e.g., 
ğ‘¥
â€‰â£
â†’
xâ†’Hypernym
â†’
?
â†’?).

Shadow rollout horizon: 
ğ‘‡
=
16
T=16 steps per probe.

Adaptive rotation: run fixed 128 (balanced), plus top-64 by recent information gain, plus 64 least-recently-used.

4) Edit-as-experiment (the only way W changes)
Proposal 
Î”
ğ‘Š
Î”W:
(a) add/strengthen â‰¤12 edges, or
(b) add one new vertex (delay/context/phase/concept/hub/macro-anchor) with â‰¤12 incident edges, or
(c) rank-1 low-rank nudge (
Î”
ğ‘ˆ
â€‰
Î”
ğ‘‰
âŠ¤
Î”UÎ”V 
âŠ¤
 ).

Shadow evaluation (queries only):

Probe-MSE: trajectory MSE vs. pre-edit on all probes.

Probe-KL: KL on a small monitored node set (tokens + all special vertices).

Type-balance: track per-territory improvement 
ğ½
ğœ
J 
Ï„
â€‹
  (TEMP/SEM/PHASE/FEAT).

Safety: spectral radius after projection, budgets, row caps.

Accept iff (all true):

Probe-MSE â†“ â‰¥ 5% on â‰¥ 2/3 probes.

Mean probe-KL â‰¤ 2Ã—10â»Â³.

Spectral/budget invariants hold.

Interference tolerance: semantic edits may not raise temporal-probe MSE >1% on >1/3 temporal probes (and vice-versa).

Cadence: â‰¤ 1 accepted edit / 500 steps. On reject: halve 
âˆ¥
Î”
ğ‘Š
âˆ¥
âˆ¥Î”Wâˆ¥ and retry â‰¤2Ã—, else discard.

Post-accept projection: rescale to spectral bound, re-cap rows, enforce off-diagonal nonnegativity.

5) Specialisation without split models
Dynamic per-row split: keep 
ğ‘˜
total
=
32
k 
total
â€‹
 =32. Maintain EMA utilities 
ğ‘ˆ
ğœ
U 
Ï„
â€‹
  by type; compute fractions 
ğ‘“
ğœ
âˆ
exp
â¡
(
ğœ‚
ğ‘ˆ
ğœ
)
f 
Ï„
â€‹
 âˆexp(Î·U 
Ï„
â€‹
 ) (Î·=5); set integer caps 
ğ‘˜
ğœ
=
max
â¡
(
4
,
âŒŠ
ğ‘“
ğœ
ğ‘˜
total
âŒ‹
)
k 
Ï„
â€‹
 =max(4,âŒŠf 
Ï„
â€‹
 k 
total
â€‹
 âŒ‹). Enforce at prune time only.

Territory fairness rule: if 
ğ½
ğœ
â€²
<
0.6
â‹…
ğ½
Ë‰
J 
Ï„ 
â€²
 
â€‹
 <0.6â‹… 
J
Ë‰
  over last N=50 edits, temporarily tighten dominant typeâ€™s cap next prune (automatic, rule-based).

6) Resonance and harmonic disambiguation (W-only)
Cycle operator: non-backtracking 
ğµ
(
ğ‘Š
)
B(W) derived on the fly from 
ğ‘Š
W.

Hutch++ moments: estimate 
ğœ‡
^
ğ‘˜
=
t
r
(
ğµ
ğ‘˜
)
Î¼
^
â€‹
  
k
â€‹
 =tr(B 
k
 ) with m=8 vectors; cadence 500 steps; 
ğ‘˜
âˆˆ
{
2
,
3
,
5
,
6
,
7
,
10
,
12
}
kâˆˆ{2,3,5,6,7,10,12}.

Sparse divisor deconvolution: 
min
â¡
ğœ‹
â‰¥
0
âˆ¥
ğ·
ğœ‹
âˆ’
ğœ‡
^
âˆ¥
2
2
+
ğœ†
âˆ¥
ğœ‹
âˆ¥
1
min 
Ï€â‰¥0
â€‹
 âˆ¥DÏ€âˆ’ 
Î¼
^
â€‹
 âˆ¥ 
2
2
â€‹
 +Î»âˆ¥Ï€âˆ¥ 
1
â€‹
  (Î»=1e-3) on the divisor lattice.

Composite-first MDL test: compare 
{
ğ‘
}
{q} vs 
{
ğ‘‘
ğ‘–
ğ‘£
ğ‘–
ğ‘ 
ğ‘œ
ğ‘Ÿ
ğ‘ 
}
{divisors} vs Null on held-out probes; require one winner.

Phase-coherence check: phases for divisors must lock to composite; else allow primes as independent rhythms.

Detune verification: shadow â€œdamp every 
ğ‘
pth tickâ€ runs; prefer hypothesis with largest hurt gap.

Materialisation: if period 
ğ‘
q accepted, add a light macro-anchor 
ğ‘€
ğ‘
M 
q
â€‹
  connected to implicated subgraph. Blacklist its prime divisors in that namespace for 10k steps unless independence proven.

7) Semantics inside W (no bolt-ons)
Concept vertices: small global pool (default â‰¤64) for abstract predicates (Fruit, Red, Company, Edible, â€¦).

Operator vertices: minimal set (Hypernym, Hyponym, Pluralize, ColorOf, PartOf, â€¦).

Grounding rule: any external cue (co-activation, image signal, dictionary/LLM suggestion) may propose token â†’ SEM â†’ concept, but acceptance depends only on probe improvements + physics gates; no parallel memory retained.

Concept-chord acceptance: average normalized energy on intended targets must â†‘ â‰¥ 0.02 with no drop on â‰¥â…” chords; temporal probes must stay within tolerance.

Polysemy: if probe signatures for token split (silhouette â‰¥ 0.25), propose sense nodes 
ğ‘¥
(
1
)
,
ğ‘¥
(
2
)
x 
(1)
 ,x 
(2)
  and context routing; accept if signature margin â†‘ â‰¥ 0.1 and gates pass.

Hubs & long wires: allow concept hubs 
ğ»
ğ¶
H 
C
â€‹
  or low-rank nudges for long-range semantic recall, under the same gate and spectral projection.

8) Data & I/O contracts (minimal)
Events stream: seq_id, step, token, namespace, weight, split.
(Optional modality tags for co-activity proposals; once edges are proposed, only 
ğ‘Š
W persists them.)

No separate semantic table: any seed concept list is for proposals only; after acceptance, concept vertices/edges live in 
ğ‘Š
W.

Checkpoints: save 
ğ‘Š
W (sparse 
ğ‘†
S + low-rank 
ğ‘ˆ
,
ğ‘‰
U,V), vertex registry, budgets, and edit witness logs (which probes improved, deltas). No other state.

9) Metrics & dashboards (readouts, not losses)
Physics: spectral radius, 
âˆ£
ğ¸
âˆ£
,
âˆ£
ğ‘‰
âˆ£
âˆ£Eâˆ£,âˆ£Vâˆ£, row cap violations (should be 0), probe-MSE/KL curves.

Resonance: 
ğœ‡
^
ğ‘˜
Î¼
^
â€‹
  
k
â€‹
 , selected 
ğœ‹
^
ğ‘
Ï€
^
  
q
â€‹
 , detune hurt gaps; subgraph Lanczos mode mass.

Semantics: concept-chord scores, invariance scores 
ğ¼
(
ğ‘¥
,
ğ‘ƒ
ğ‘˜
)
=
min
â¡
ğ‘—
âˆˆ
ğ½
ğ‘˜
I(x,P 
k
â€‹
 )=min 
jâˆˆJ 
k
â€‹
 
â€‹
  energy
(
ğ‘ƒ
ğ‘˜
âˆ£
ğ‘ˆ
ğ‘—
(
ğ‘¥
)
)
(P 
k
â€‹
 âˆ£U 
j
â€‹
 (x)), sense separation margins.

Territories: per-type utilities 
ğ‘ˆ
ğœ
U 
Ï„
â€‹
 , improvements 
ğ½
ğœ
J 
Ï„
â€‹
 , current 
ğ‘˜
ğœ
k 
Ï„
â€‹
  splits.

10) Phased plan (ocean-capable POC)
Phase-0 (skeleton):
Implement 
ğ‘Š
W core (S+UVáµ€), probes, edit loop, spectral projection, budgets, delay nodes. Validate on synthetic periodic/aperiodic streams. Exit when edits consistently pass gates with no collapses.

Phase-1 (resonance/macros):
Add 
ğµ
(
ğ‘Š
)
B(W)+Hutch++, sparse deconvolution, composite-first MDL, phase-coherence, detune. Materialise 
ğ‘€
ğ‘
M 
q
â€‹
  anchors and blacklist divisors appropriately.

Phase-2 (higher-order + semantics):
Promote context nodes, enable phase motifs, introduce concept vertices and semantic chords, allow hubs/low-rank â€œlong wires,â€ polysemy splits. Maintain interference guards and territory fairness.

11) Defaults to pin (implementation constants)
ğœŒ
\*
=
0.98
Ï 
\*
 =0.98, 
ğ›¿
=
0.02
Î´=0.02, 
ğ‘˜
row
=
32
k 
row
â€‹
 =32, 
âˆ£
ğ¸
âˆ£
â‰¤
20
âˆ£
ğ‘‰
âˆ£
âˆ£Eâˆ£â‰¤20âˆ£Vâˆ£, 
ğ‘Ÿ
=
8
r=8, 
ğ¾
=
8
K=8 delay nodes.

Probe pool 
=
256
=256, horizon 
ğ‘‡
=
16
T=16; cadence: evaluate at every shadow run.

Hutch++ cadence 500 steps, 
ğ‘š
=
8
m=8 vectors, 
ğ‘˜
âˆˆ
{
2
,
3
,
5
,
6
,
7
,
10
,
12
}
kâˆˆ{2,3,5,6,7,10,12}, Î»=1e-3.

Edit cadence â‰¤1/500 steps; proposal size â‰¤12 edges or 1 node + â‰¤12 edges; retry Ã—2 on backoff.

Acceptance thresholds: MSE â†“ â‰¥5% (â‰¥â…” probes), mean KL â‰¤2e-3; semantic chord â†‘ â‰¥0.02; temporal-semantic interference tolerance Â±1% on â‰¤1/3 probes.

Territory split: start 
ğ‘˜
TEMP
=
24
k 
TEMP
â€‹
 =24, 
ğ‘˜
SEM
=
8
k 
SEM
â€‹
 =8; adapt via multiplicative-weights at prune.

12) Invariants and kill-switches
Hard invariants: no NaNs; off-diagonal weights 
â‰¥
0
â‰¥0; row 
â„“
1
â‰¤
1
â„“ 
1
â€‹
 â‰¤1 pre-damping; 
ğœŒ
(
ğ‘Š
)
â‰¤
1
Ï(W)â‰¤1 after projection; 
âˆ£
ğ¸
âˆ£
â‰¤
20
âˆ£
ğ‘‰
âˆ£
âˆ£Eâˆ£â‰¤20âˆ£Vâˆ£.

Kill-switch: if two successive accepted edits require >1.1Ã— spectral scaling or violate interference tolerance, freeze structural edits for 5k steps and alert.

13) Open choices to confirm (before coding)
Global vs per-namespace special nodes: default global with namespace-specific edges.

Sense split policy: maximum senses per token (default 2).

Concept pool: initial list and cap (default â‰¤64).

Operator vertices: minimal set to include at Phase-2.

Probe monitored node set: which tokens + all special nodes; size cap (e.g., 512).

14) What this answers (and what it doesnâ€™t)
Answers: harmonic interference (non-backtracking + deconv + detune), semantic abyss (concept vertices + probes), module drift (typed paths within 
ğ‘Š
W), resource starvation (dynamic per-row split), collapse (gated edits + spectral safety).

Doesnâ€™t: conjure semantics without grounding; guarantee human-level abstractionâ€”only that any new capability must be realised as structure in 
ğ‘Š
W and justified by probe behaviour.



----initial:----


# UPCA3
Its About Time!
A time-first, resonance-driven UPCA (mathematical spec)
0) Observables and timeline
Discrete time tâˆˆNt\in\mathbb{N}tâˆˆN. Streamed tokens/events xtâˆˆVx_t\in\mathcal{V}xtâ€‹âˆˆV (words/objs/evts).
Define binary indicators yt(j)=1[xt=j]y_t(j)=\mathbf{1}[x_t=j]ytâ€‹(j)=1[xtâ€‹=j]. Let a window Wt={tâˆ’W+1,â€¦,t}\mathcal{W}_t=\{t-W+1,\dots,t\}Wtâ€‹={tâˆ’W+1,â€¦,t}.
Delays: for any ordered pair (iâ€‰â£â†’â€‰â£j)(i\!\to\!j)(iâ†’j), let Î”t(iâ†’j)\Delta_t^{(i\to j)}Î”t(iâ†’j)â€‹ be the number of steps between the latest iii before ttt and the next jjj at ttt (well-defined when xt=jx_t=jxtâ€‹=j and some iii occurred earlier).
1) ME (Detail): lag-typed transitions + semi-Markov â€œwaitingâ€
1.1 Lag-typed next-token model
Choose a maximum lag LLL. For each lag â„“âˆˆ{1,â€¦,L}\ell\in\{1,\dots,L\}â„“âˆˆ{1,â€¦,L} maintain a sparse, row-stochastic matrix A(â„“)âˆˆRâ‰¥0âˆ£Vâˆ£Ã—âˆ£Vâˆ£A^{(\ell)}\in\mathbb{R}_{\ge0}^{|\mathcal{V}|\times|\mathcal{V}|}A(â„“)âˆˆRâ‰¥0âˆ£Vâˆ£Ã—âˆ£Vâˆ£â€‹ with rows indexed by past token and columns by next token.
Let Î²â„“â‰¥0\beta_\ell\ge0Î²â„“â€‹â‰¥0 with âˆ‘â„“Î²â„“=1\sum_\ell \beta_\ell=1âˆ‘â„“â€‹Î²â„“â€‹=1. Define logits
zt(j)=âˆ‘â„“=1LÎ²â„“â€‰Axtâˆ’â„“,â€‰j(â„“),pME(xt=jâˆ£Htâˆ’1)=expâ¡zt(j)âˆ‘kâˆˆVexpâ¡zt(k).z_t(j)=\sum_{\ell=1}^L \beta_\ell\, A^{(\ell)}_{x_{t-\ell},\,j}\quad,\qquad p_{\text{ME}}(x_t=j\mid \mathcal{H}_{t-1})=\frac{\exp z_t(j)}{\sum_{k\in\mathcal{V}}\exp z_t(k)}.ztâ€‹(j)=â„“=1âˆ‘Lâ€‹Î²â„“â€‹Axtâˆ’â„“â€‹,j(â„“)â€‹,pMEâ€‹(xtâ€‹=jâˆ£Htâˆ’1â€‹)=âˆ‘kâˆˆVâ€‹expztâ€‹(k)expztâ€‹(j)â€‹.
(Equivalently, view A(â„“)A^{(\ell)}A(â„“) as additive experts over lags.)
1.2 Variable-delay (semi-Markov) waiting for continuations
For each ordered pair (iâ€‰â£â†’â€‰â£j)(i\!\to\!j)(iâ†’j), define a discrete-time hazard hij(d)âˆˆ(0,1)h_{ij}(d)\in(0,1)hijâ€‹(d)âˆˆ(0,1) over delays dâˆˆ{1,2,â€¦â€‰}d\in\{1,2,\dots\}dâˆˆ{1,2,â€¦},
hij(d)=Ïƒâ€‰â£(Î¸ijâŠ¤Ïˆ(d)â€‰+â€‰uiâŠ¤vj),Ïƒ(a)=11+eâˆ’a,h_{ij}(d)=\sigma\!\big(\theta_{ij}^\top \psi(d) \,+\, u_i^\top v_j\big),\quad \sigma(a)=\tfrac{1}{1+e^{-a}},hijâ€‹(d)=Ïƒ(Î¸ijâŠ¤â€‹Ïˆ(d)+uiâŠ¤â€‹vjâ€‹),Ïƒ(a)=1+eâˆ’a1â€‹,
with basis Ïˆ(d)\psi(d)Ïˆ(d) (e.g., [1,logâ¡d,d,dâˆ’1][1,\log d, d, d^{-1}][1,logd,d,dâˆ’1]) and pair embeddings ui,vju_i,v_juiâ€‹,vjâ€‹. The survival and delay pmf are
Sij(Î”)=âˆd=1Î”âˆ’1(1âˆ’hij(d)),fij(Î”)=Sij(Î”)â€‰hij(Î”).S_{ij}(\Delta)=\prod_{d=1}^{\Delta-1}\big(1-h_{ij}(d)\big),\qquad f_{ij}(\Delta)=S_{ij}(\Delta)\,h_{ij}(\Delta).Sijâ€‹(Î”)=d=1âˆÎ”âˆ’1â€‹(1âˆ’hijâ€‹(d)),fijâ€‹(Î”)=Sijâ€‹(Î”)hijâ€‹(Î”).
When xt=jx_t=jxtâ€‹=j follows the most recent iii with delay Î”=Î”t(iâ†’j)\Delta=\Delta_t^{(i\to j)}Î”=Î”t(iâ†’j)â€‹, the time-likelihood term is logâ¡fij(Î”)\log f_{ij}(\Delta)logfijâ€‹(Î”).
1.3 ME instantaneous loss and online gradients
Instantaneous negative log-likelihood (NLL) with regularization:
LtME=âˆ’logâ¡pME(xtâˆ£Htâˆ’1)â€…â€Šâˆ’â€‰â£â€‰â£âˆ‘i:Â last(i)<tâ€‰â£â€‰â£1[xt=j]logâ¡fijâ€‰â£(Î”t(iâ†’j))â€…â€Š+â€…â€ŠÎ»Aâ€‰â£âˆ‘â„“â€‰â£âˆ¥A(â„“)âˆ¥1â€…â€Š+â€…â€ŠÎ»Î¸âˆ¥Î˜âˆ¥22.\mathcal{L}^{\text{ME}}_t = -\log p_{\text{ME}}(x_t\mid \mathcal{H}_{t-1}) \;-\!\!\sum_{i:\ \text{last}(i)<t}\!\!\mathbf{1}[x_t=j]\log f_{ij}\!\big(\Delta_t^{(i\to j)}\big) \;+\;\lambda_A\!\sum_{\ell}\!\|A^{(\ell)}\|_{1} \;+\;\lambda_\theta\|\Theta\|_2^2.LtMEâ€‹=âˆ’logpMEâ€‹(xtâ€‹âˆ£Htâˆ’1â€‹)âˆ’i:Â last(i)<tâˆ‘â€‹1[xtâ€‹=j]logfijâ€‹(Î”t(iâ†’j)â€‹)+Î»Aâ€‹â„“âˆ‘â€‹âˆ¥A(â„“)âˆ¥1â€‹+Î»Î¸â€‹âˆ¥Î˜âˆ¥22â€‹.
For A(â„“)A^{(\ell)}A(â„“) (additive-expert form), the stochastic gradient is
âˆ‚LtMEâˆ‚Ai,j(â„“)=(â€‰pME(jâˆ£Htâˆ’1)âˆ’yt(j)â€‰)â€‰1[xtâˆ’â„“=i]â€…â€Š+â€…â€ŠÎ»Aâ€‰signâ€‰â£(Ai,j(â„“)).\frac{\partial \mathcal{L}^{\text{ME}}_t}{\partial A^{(\ell)}_{i,j}} = \big(\,p_{\text{ME}}(j\mid\mathcal{H}_{t-1})-y_t(j)\,\big)\,\mathbf{1}[x_{t-\ell}=i] \;+\;\lambda_A\,\text{sign}\!\big(A^{(\ell)}_{i,j}\big).âˆ‚Ai,j(â„“)â€‹âˆ‚LtMEâ€‹â€‹=(pMEâ€‹(jâˆ£Htâˆ’1â€‹)âˆ’ytâ€‹(j))1[xtâˆ’â„“â€‹=i]+Î»Aâ€‹sign(Ai,j(â„“)â€‹).
For hazard parameters Î¸ij\theta_{ij}Î¸ijâ€‹,
âˆ‚âˆ‚Î¸ijâ€‰â£(âˆ’logâ¡fij(Î”))=âˆ’(1âˆ’hij(Î”))Ïˆ(Î”)+âˆ‘d=1Î”âˆ’1hij(d)1âˆ’hij(d)â€‰Ïˆ(d).\frac{\partial}{\partial \theta_{ij}} \!\left(-\log f_{ij}(\Delta)\right) = -\Big(1-h_{ij}(\Delta)\Big)\psi(\Delta) +\sum_{d=1}^{\Delta-1}\frac{h_{ij}(d)}{1-h_{ij}(d)}\,\psi(d).âˆ‚Î¸ijâ€‹âˆ‚â€‹(âˆ’logfijâ€‹(Î”))=âˆ’(1âˆ’hijâ€‹(Î”))Ïˆ(Î”)+d=1âˆ‘Î”âˆ’1â€‹1âˆ’hijâ€‹(d)hijâ€‹(d)â€‹Ïˆ(d).
Use eligibility traces eij(t)=Î³eij(tâˆ’1)+1[xt=i]e_{ij}(t)=\gamma e_{ij}(t-1)+\mathbf{1}[x_t=i]eijâ€‹(t)=Î³eijâ€‹(tâˆ’1)+1[xtâ€‹=i] to gate updates only for recently active anchors iii.
2) MA (Abstract): resonance, periods, and cyclic macros
2.1 Autocorrelation and trace-moments on a streaming operator
Build a row-stochastic operator WtW_tWtâ€‹ from the recent A(â„“)A^{(\ell)}A(â„“) (e.g., Wt=âˆ‘â„“â‰¤Lâ€²Î²â„“Î (â„“)W_t=\sum_{\ell\le L'}\beta_\ell \Pi^{(\ell)}Wtâ€‹=âˆ‘â„“â‰¤Lâ€²â€‹Î²â„“â€‹Î (â„“), where Î (â„“)\Pi^{(\ell)}Î (â„“) is the empirical transition at lag â„“\ellâ„“ over Wt\mathcal{W}_tWtâ€‹). Define closed-walk moments
Î¼k(t)=trâ¡(Wtk),Î k(t)=1kâˆ‘dâˆ£kÎ¼(d)â€‰Î¼k/d(t)(MoÂ¨biusÂ inversion),\mu_k(t)=\operatorname{tr}(W_t^k),\qquad \Pi_k(t)=\frac{1}{k}\sum_{d\mid k}\mu(d)\,\mu_{k/d}(t)\quad\text{(MÃ¶bius inversion)},Î¼kâ€‹(t)=tr(Wtkâ€‹),Î kâ€‹(t)=k1â€‹dâˆ£kâˆ‘â€‹Î¼(d)Î¼k/dâ€‹(t)(MoÂ¨biusÂ inversion),
with MÃ¶bius Î¼(â‹…)\mu(\cdot)Î¼(â‹…). A â€œprime toneâ€ at ppp is a spike in Î p(t)\Pi_p(t)Î pâ€‹(t).
2.2 Ramanujan projections on the timeline
Let ata_tatâ€‹ be an activation (e.g., token counts or NLL spike). For modulus qqq,
cq(n)=âˆ‘1â‰¤aâ‰¤qgcdâ¡(a,q)=1e2Ï€ian/q,Rq(Ï„)=1Tâˆ£âˆ‘s=Ï„Ï„+Tâˆ’1asâ€‰cq(s)âˆ£.c_q(n)=\sum_{\substack{1\le a\le q\\ \gcd(a,q)=1}} e^{2\pi i a n/q},\qquad R_q(\tau)=\frac{1}{T}\Big|\sum_{s=\tau}^{\tau+T-1} a_s\, c_q(s)\Big|.cqâ€‹(n)=1â‰¤aâ‰¤qgcd(a,q)=1â€‹âˆ‘â€‹e2Ï€ian/q,Rqâ€‹(Ï„)=T1â€‹â€‹s=Ï„âˆ‘Ï„+Tâˆ’1â€‹asâ€‹cqâ€‹(s)â€‹.
For primes ppp, large Rp(Ï„)R_p(\tau)Rpâ€‹(Ï„) indicates prime-period energy in window [Ï„,Ï„+T)[\tau,\tau+T)[Ï„,Ï„+T).
2.3 Cyclic ABS macros (phase models)
Introduce latent macros m=1,â€¦,Mm=1,\dots,Mm=1,â€¦,M, each with prime period pmp_mpmâ€‹ and phase Ï•m,tâˆˆ{0,â€¦,pmâ€‰â£âˆ’â€‰â£1}\phi_{m,t}\in\{0,\dots,p_m\!-\!1\}Ï•m,tâ€‹âˆˆ{0,â€¦,pmâ€‹âˆ’1} evolving as Ï•m,t+1=(Ï•m,t+1)â€Šmodâ€Špm\phi_{m,t+1}=(\phi_{m,t}+1)\bmod p_mÏ•m,t+1â€‹=(Ï•m,tâ€‹+1)modpmâ€‹. Each macro has phase-conditional emissions UmâˆˆRpmÃ—âˆ£Vâˆ£U_m\in\mathbb{R}^{p_m\times|\mathcal{V}|}Umâ€‹âˆˆRpmâ€‹Ã—âˆ£Vâˆ£:
pm(xt=jâˆ£Ï•m,t)=expâ¡Um[Ï•m,t,j]âˆ‘kexpâ¡Um[Ï•m,t,k].p_m(x_t=j\mid \phi_{m,t})=\frac{\exp U_m[\phi_{m,t},j]}{\sum_k \exp U_m[\phi_{m,t},k]}.pmâ€‹(xtâ€‹=jâˆ£Ï•m,tâ€‹)=âˆ‘kâ€‹expUmâ€‹[Ï•m,tâ€‹,k]expUmâ€‹[Ï•m,tâ€‹,j]â€‹.
Phase filtering (circular HMM) uses
Î±m,t(Ï•)âˆÎ±m,tâˆ’1(Ï•â€‰â£âˆ’â€‰â£1)â€…â€Špm(xtâˆ£Ï•),wm(t)âˆexpâ¡â€‰â£(Îºâ‹…Î pm(t)+Î·â‹…Rpm(Ï„t)).\alpha_{m,t}(\phi)\propto \alpha_{m,t-1}(\phi\!-\!1)\;p_m(x_t\mid \phi),\quad w_m(t)\propto \exp\!\Big(\kappa\cdot \Pi_{p_m}(t)+\eta\cdot R_{p_m}(\tau_t)\Big).Î±m,tâ€‹(Ï•)âˆÎ±m,tâˆ’1â€‹(Ï•âˆ’1)pmâ€‹(xtâ€‹âˆ£Ï•),wmâ€‹(t)âˆexp(Îºâ‹…Î pmâ€‹â€‹(t)+Î·â‹…Rpmâ€‹â€‹(Ï„tâ€‹)).
2.4 MEâ€“MA mixture
p(xtâˆ£Htâˆ’1)=(1âˆ’Î»t)â€‰pME(xtâˆ£Htâˆ’1)â€…â€Š+â€…â€ŠÎ»tâ€‰âˆ‘mÏ€m(t)â€‰pm(xtâˆ£Ï•m,t),p(x_t\mid \mathcal{H}_{t-1}) = (1-\lambda_t)\,p_{\text{ME}}(x_t\mid\mathcal{H}_{t-1}) \;+\;\lambda_t\,\sum_{m} \pi_m(t)\, p_m(x_t\mid \phi_{m,t}),p(xtâ€‹âˆ£Htâˆ’1â€‹)=(1âˆ’Î»tâ€‹)pMEâ€‹(xtâ€‹âˆ£Htâˆ’1â€‹)+Î»tâ€‹mâˆ‘â€‹Ï€mâ€‹(t)pmâ€‹(xtâ€‹âˆ£Ï•m,tâ€‹),
where Ï€m(t)=wm(t)âˆ‘mâ€²wmâ€²(t)\pi_m(t)=\frac{w_m(t)}{\sum_{m'}w_{m'}(t)}Ï€mâ€‹(t)=âˆ‘mâ€²â€‹wmâ€²â€‹(t)wmâ€‹(t)â€‹ and Î»tâˆˆ[0,1]\lambda_t\in[0,1]Î»tâ€‹âˆˆ[0,1] is AMC-controlled (below).
3) AMC: arbitration, consolidation, and trust-region stability
3.1 Surprise, persistence, and off-tone gating
Define instantaneous surprise
Stâ€…â€Š=â€…â€Šâˆ’logâ¡p(xtâˆ£Htâˆ’1)â€…â€Šâˆ’â€‰â£â€‰â£âˆ‘i:last(i)<tâ€‰â£1[xt=j]logâ¡fij(Î”t(iâ†’j)).\mathcal{S}_t \;=\; -\log p(x_t\mid\mathcal{H}_{t-1}) \;-\!\!\sum_{i:\text{last}(i)<t}\!\mathbf{1}[x_t=j]\log f_{ij}(\Delta_t^{(i\to j)}).Stâ€‹=âˆ’logp(xtâ€‹âˆ£Htâˆ’1â€‹)âˆ’i:last(i)<tâˆ‘â€‹1[xtâ€‹=j]logfijâ€‹(Î”t(iâ†’j)â€‹).
Let SË‰t=EMAÏ„(St)\bar{\mathcal{S}}_t=\text{EMA}_\tau(\mathcal{S}_t)SË‰tâ€‹=EMAÏ„â€‹(Stâ€‹) and define off-tone factor for dominant prime pâ‹†p^\starpâ‹† (if present):
gpâ‹†(Î”)={Ï,Î”â€Šmodâ€Špâ‹†âˆˆ{0,1,pâ‹†â€‰â£âˆ’â€‰â£1}Ïƒ,otherwise(Ï>1>Ïƒ>0).g_{p^\star}(\Delta)= \begin{cases} \rho,& \Delta\bmod p^\star\in\{0,1,p^\star\!-\!1\}\\ \sigma,& \text{otherwise} \end{cases}\quad (\rho>1>\sigma>0).gpâ‹†â€‹(Î”)={Ï,Ïƒ,â€‹Î”modpâ‹†âˆˆ{0,1,pâ‹†âˆ’1}otherwiseâ€‹(Ï>1>Ïƒ>0).
Use gpâ‹†g_{p^\star}gpâ‹†â€‹ to scale learning rates for updates induced by delays Î”\DeltaÎ”.
3.2 Arbitration objective and control
AMC chooses Î»t\lambda_tÎ»tâ€‹ and (occasionally) spawns/merges macros by minimizing
LtAMC=SË‰t+Î³câ€‰(#params)âŸcomplexityâˆ’Îºpâ€‰maxâ¡pâˆˆPÎ p(t)âˆ’Îºrâ€‰maxâ¡pâˆˆPRp(Ï„t),\mathcal{L}^{\text{AMC}}_t = \bar{\mathcal{S}}_t + \gamma_c\,\underbrace{\big(\#\text{params}\big)}_{\text{complexity}} - \kappa_p\,\max_{p\in\mathcal{P}}\Pi_p(t) - \kappa_r\,\max_{p\in\mathcal{P}}R_p(\tau_t),LtAMCâ€‹=SË‰tâ€‹+Î³câ€‹complexity(#params)â€‹â€‹âˆ’Îºpâ€‹pâˆˆPmaxâ€‹Î pâ€‹(t)âˆ’Îºrâ€‹pâˆˆPmaxâ€‹Rpâ€‹(Ï„tâ€‹),
subject to a trust-region stability constraint over a guarded query set Qt\mathcal{Q}_tQtâ€‹ (high-support contexts):
1âˆ£Qtâˆ£âˆ‘qâˆˆQtDKLâ€‰â£(Ppre(â‹…âˆ£q)â€‰âˆ¥â€‰Ppost(â‹…âˆ£q))Â â‰¤Â Ïµ.\frac{1}{|\mathcal{Q}_t|}\sum_{q\in\mathcal{Q}_t} D_{\mathrm{KL}}\!\left(P_{\text{pre}}(\cdot\mid q)\,\big\|\,P_{\text{post}}(\cdot\mid q)\right)\ \le\ \epsilon.âˆ£Qtâ€‹âˆ£1â€‹qâˆˆQtâ€‹âˆ‘â€‹DKLâ€‹(Ppreâ€‹(â‹…âˆ£q)â€‹Ppostâ€‹(â‹…âˆ£q))Â â‰¤Â Ïµ.
Any prune/merge/add that violates the bound is rejected or softened. (This formalizes â€œdonâ€™t delete if it tanks success.â€)
3.3 Macro creation/merging criteria
Spawn macro mmm with prime pmp_mpmâ€‹ when both hold on Wt\mathcal{W}_tWtâ€‹:
Î pm(t)â‰¥Ï„Î andRpm(Ï„t)â‰¥Ï„R,\Pi_{p_m}(t) \ge \tau_{\Pi}\quad\text{and}\quad R_{p_m}(\tau_t)\ge \tau_{R},Î pmâ€‹â€‹(t)â‰¥Ï„Î â€‹andRpmâ€‹â€‹(Ï„tâ€‹)â‰¥Ï„Râ€‹,
and the expected risk decreases:
Î”E[SË‰]â€…â€Šâ‰ˆâ€…â€ŠEWtâ€‰â£[âˆ’logâ¡â€‰â£((1âˆ’Î»)pME+Î»pm)]âŸwithÂ macroâ€…â€Šâˆ’â€…â€ŠEWtâ€‰â£[âˆ’logâ¡pME]âŸbaselineâ€…â€Š<â€…â€Šâˆ’Ï„gain,\Delta\mathbb{E}[\bar{\mathcal{S}}]\;\approx\; \underbrace{\mathbb{E}_{\mathcal{W}_t}\!\left[-\log\!\big((1-\lambda)p_{\text{ME}}+\lambda p_m\big)\right]}_{\text{with macro}} \;-\; \underbrace{\mathbb{E}_{\mathcal{W}_t}\!\left[-\log p_{\text{ME}}\right]}_{\text{baseline}} \;<\; -\tau_{\text{gain}},Î”E[SË‰]â‰ˆwithÂ macroEWtâ€‹â€‹[âˆ’log((1âˆ’Î»)pMEâ€‹+Î»pmâ€‹)]â€‹â€‹âˆ’baselineEWtâ€‹â€‹[âˆ’logpMEâ€‹]â€‹â€‹<âˆ’Ï„gainâ€‹,
while satisfying the trust-region constraint.
4) Regularizers that encode â€œresonance-firstâ€
Spectral prior: encourage energy at discovered primes:
Rspec=âˆ’Î±Î âˆ‘pâˆˆPÎ p(t)â€…â€Šâˆ’â€…â€ŠÎ±Râˆ‘pâˆˆPRp(Ï„t).\mathcal{R}_{\text{spec}}=-\alpha_{\Pi}\sum_{p\in\mathcal{P}}\Pi_p(t)\;-\;\alpha_{R}\sum_{p\in\mathcal{P}}R_p(\tau_t).Rspecâ€‹=âˆ’Î±Î â€‹pâˆˆPâˆ‘â€‹Î pâ€‹(t)âˆ’Î±Râ€‹pâˆˆPâˆ‘â€‹Rpâ€‹(Ï„tâ€‹).
Capacity control: Î»Aâˆ¥Aâˆ¥1\lambda_A\|A\|_1Î»Aâ€‹âˆ¥Aâˆ¥1â€‹ (sparseness) and phase-smoothness âˆ‘mâˆ‘Ï•âˆ¥Um[Ï•]âˆ’Um[Ï•â€‰â£âˆ’â€‰â£1]âˆ¥22\sum_m\sum_\phi \|U_m[\phi]-U_m[\phi\!-\!1]\|_2^2âˆ‘mâ€‹âˆ‘Ï•â€‹âˆ¥Umâ€‹[Ï•]âˆ’Umâ€‹[Ï•âˆ’1]âˆ¥22â€‹.
5) Global objective and update sketch
Across time, minimize
minâ¡Î˜Â âˆ‘t(LtMEâ€…â€Š+â€…â€Š(1âˆ’Î»t)â€‰[âˆ’logâ¡pME(xtâˆ£Htâˆ’1)]âŸalreadyÂ inÂ LtMEâ€…â€Š+â€…â€ŠÎ»tâ€‰[âˆ’logâ¡â€‰â£âˆ‘mÏ€m(t)pm(xtâˆ£Ï•m,t)])+Rspec+complexity,\min_{\Theta}\ \sum_{t}\Big(\mathcal{L}^{\text{ME}}_t \;+\; (1-\lambda_t)\,\underbrace{\big[-\log p_{\text{ME}}(x_t\mid\mathcal{H}_{t-1})\big]}_{\text{already in }\mathcal{L}^{\text{ME}}_t} \;+\;\lambda_t\,\big[-\log\!\sum_m \pi_m(t)p_m(x_t\mid\phi_{m,t})\big]\Big) +\mathcal{R}_{\text{spec}} + \text{complexity},Î˜minâ€‹Â tâˆ‘â€‹(LtMEâ€‹+(1âˆ’Î»tâ€‹)alreadyÂ inÂ LtMEâ€‹[âˆ’logpMEâ€‹(xtâ€‹âˆ£Htâˆ’1â€‹)]â€‹â€‹+Î»tâ€‹[âˆ’logmâˆ‘â€‹Ï€mâ€‹(t)pmâ€‹(xtâ€‹âˆ£Ï•m,tâ€‹)])+Rspecâ€‹+complexity,
subject to the AMC trust region at each structural change. Stochastic online updates:
Î˜â†Î˜âˆ’Î·tâ€‰gpâ‹†(Î”)â€‰âˆ‡Î˜(instantaneousÂ loss),\Theta \leftarrow \Theta - \eta_t\,g_{p^\star}(\Delta)\,\nabla_\Theta \big(\text{instantaneous loss}\big),Î˜â†Î˜âˆ’Î·tâ€‹gpâ‹†â€‹(Î”)âˆ‡Î˜â€‹(instantaneousÂ loss),
with EMA-based steps for Î»t,Ï€m(t)\lambda_t,\pi_m(t)Î»tâ€‹,Ï€mâ€‹(t) and periodic structural moves (spawn/merge/prune) only if the KL-bound holds.
6) What this formalization guarantees
Waiting state: encoded by fij(Î”)f_{ij}(\Delta)fijâ€‹(Î”) (semi-Markov); â€œlightningâ†’thunderâ€ is a high-hazard pair at characteristic Î”\DeltaÎ”.
Resonance-first: periods are detected by Î p(t)\Pi_p(t)Î pâ€‹(t) (closed-walk primitives) and RpR_pRpâ€‹ (Ramanujan timeline projection); macros bind them via phases.
Chunk-agnostic: whether input is 1-gram/2-gram/triple, the same lag/hazard machinery applies.
Stability: the KL trust region makes â€œwhat-ifâ€ deletions mathematically safe; spectral priors prevent 2-cycle swamping and favor prime tones.


Unified Scaffold (shared state): holds lag experts 
ğ´
(
â„“
)
A 
(â„“)
 , delay hazards 
â„
ğ‘–
ğ‘—
(
ğ‘‘
)
h 
ij
â€‹
 (d)/
ğ‘“
ğ‘–
ğ‘—
(
Î”
)
f 
ij
â€‹
 (Î”), resonance summaries 
ğœ‡
ğ‘˜
,
Î 
ğ‘
Î¼ 
k
â€‹
 ,Î  
p
â€‹
 , Ramanujan windows 
ğ‘…
ğ‘
R 
p
â€‹
 , the macro library (prime-period, phase HMMs), EMAs, and KL trust-region snapshots. Itâ€™s the only source of truth each module reads/writes.

ME (Detail Engine): fast, token-time updates.
Inputs: recent context/lags; active â€œwaitingâ€ pairs.
Writes: 
ğ´
(
â„“
)
A 
(â„“)
 , hazard params 
ğœƒ
ğ‘–
ğ‘—
,
ğ‘¢
ğ‘–
,
ğ‘£
ğ‘—
Î¸ 
ij
â€‹
 ,u 
i
â€‹
 ,v 
j
â€‹
 .
Outputs: 
ğ‘
ME
(
ğ‘¥
ğ‘¡
â€‰â£
âˆ£
â€‰â£
ğ»
ğ‘¡
âˆ’
1
)
p 
ME
â€‹
 (x 
t
â€‹
 âˆ£H 
tâˆ’1
â€‹
 ), delay pmf 
ğ‘“
ğ‘–
ğ‘—
(
Î”
)
f 
ij
â€‹
 (Î”), uncertainty.
Scope: predicts what/when next; does not create structures.

MA (Abstract Engine): medium-tempo structure discovery.
Inputs: MEâ€™s 
ğ´
(
â„“
)
A 
(â„“)
  (to build 
ğ‘Š
ğ‘¡
W 
t
â€‹
 ), timeline activations.
Writes: 
ğœ‡
ğ‘˜
,
Î 
ğ‘
,
ğ‘…
ğ‘
Î¼ 
k
â€‹
 ,Î  
p
â€‹
 ,R 
p
â€‹
 ; creates/updates cyclic macros (period 
ğ‘
p, phase 
ğœ™
Ï•, emissions 
ğ‘ˆ
U).
Outputs: macro likelihoods 
ğ‘
ğ‘š
(
ğ‘¥
ğ‘¡
â€‰â£
âˆ£
â€‰â£
ğœ™
)
p 
m
â€‹
 (x 
t
â€‹
 âˆ£Ï•), prime evidence.
Scope: detects/resolves rhythms; no arbitration.

AMC (Arbiter): slow, supervisory control with safety.
Inputs: surprise/NLL, MA prime evidence, complexity.
Actions: sets mix 
ğœ†
ğ‘¡
Î» 
t
â€‹
  (MEâ†”MA), scales learning by prime gating 
ğ‘”
ğ‘
g 
p
â€‹
 ; spawns/merges/prunes macros under a KL trust-region; accepts/reverts changes.
Scope: stability, capacity, policy.

Clear interfaces:
MEâ†’MA: provides 
ğ‘Š
ğ‘¡
W 
t
â€‹
  (from 
ğ´
(
â„“
)
A 
(â„“)
 ) for spectra.
MAâ†’AMC: prime/period signals and candidate macros.
AMCâ†’ME/MA: 
ğœ†
ğ‘¡
Î» 
t
â€‹
 , learning-rate gates, and structural decisions.
Scaffold enforces versioning and KL-bounded commits.

Phase 0 â€” Proof-of-Concept â€œcore-5â€ (get a running loop fast)

src/streamupca/config.py

load_config(path) -> Config

validate(Config) -> None

freeze(Config) -> FrozenConfig (immutable view)

src/streamupca/data/dataloaders.py

stream_events(path, *, namespaces=None, split="train") -> Iterator[Event]

window(buffer, W) -> Context (last W tokens + indices)

batcher(iterable, n) -> Iterator[List[Event]] (optional)

src/streamupca/scaffold/state.py

class ScaffoldState:

from_config(cfg) -> ScaffoldState

get_A(lag:int) -> SparseRowMap / set_A(lag, row, col, val)

ema_update(name:str, value:float, tau:float)

checkpoint(save_path) / restore(load_path)

metrics_snapshot() -> Dict[str, float]

src/streamupca/models/me_lag.py (lag-experts only)

class LagExperts:

predict_proba(ctx:Context) -> Probs

nll(event:Event, ctx:Context) -> float

sgd_update(event, ctx, lr:float)

regularize(l1:float)

export_params() / import_params()

src/streamupca/runners/train_stream.py

train(cfg, events_path) â€” main loop: read â†’ predict â†’ loss â†’ update â†’ log

evaluate_next_token(cfg, events_path) -> Dict[str, float]

log_step(step, metrics:Dict)

POC exit criteria: can ingest events.csv, learn unigramâ†’lag patterns, and report Next-Token NLL, Acc@1/5, EMAs. Nothing else.

Phase 1 â€” â€œWaitingâ€ and timing (semi-Markov hazards)
6) src/streamupca/models/hazard_semi_markov.py

class HazardModel:

hazard(i,j,d) -> float

survival(i,j,Î”) -> float

pmf(i,j,Î”) -> float

loglik(i,j,Î”) -> float

update(i,j,Î”, lr) (with eligibility traces)

start_trace(i, t) / end_trace(i, t)

Integrations

Runner: add delay-NLL to loss and logging

State: store u_i, v_j, Î¸_{ij} banks and traces

Phase 2 â€” Resonance probes (no structure changes yet)
8) src/streamupca/models/resonance.py

build_W(state, lags:List[int]) -> SparseOp

trace_moments(W, ks:Iterable[int]) -> Dict[k, Î¼_k]

mobius_invert(mu:Dict[int,float]) -> Dict[k, Î _k]

ramanujan_window(a_t:Sequence[float], q:int, T:int) -> float

update_resonance_cache(state, stats)

src/streamupca/eval/metrics_resonance.py

prime_peaks(Î :Dict) -> List[(p,score)]

timeline_energy(R) -> Dict[p, float]

Milestone: logs show stable Î¼_k/Î _p and Ramanujan energies alongside accuracy.

Phase 3 â€” Minimal AMC (safety only, no macros yet)
10) src/streamupca/scaffold/trust_region.py

snapshot_predictor(state, query_set) -> DistMap

kl_on_queries(pre:DistMap, post:DistMap) -> float

select_query_set(buffer, k:int) -> List[Context]

src/streamupca/amc/controller.py

class AMCController:

choose_lambda(metrics, resonance) -> float

propose_change(state) -> Change (noop initially)

apply_with_kl_guard(state, change, Îµ) -> bool

Milestone: KL guard wired; dry-runs confirm no catastrophic drops.

Phase 4 â€” Cyclic macros (prime-period ABS) and mixing
12) src/streamupca/models/macros.py

class PhaseMacro:

step_phase() / reset_phase()

emission_prob(token) -> float

update_emission(token, lr)

class MacroLibrary:

spawn_from_peak(p:int, seeds:List[token]) -> MacroId

score(token) -> Dict[macro, prob]

merge_or_prune(criteria)

src/streamupca/models/mixer.py

mix_prob(p_me, p_macros, Î») -> Probs

blend_loss(nll_me, nll_macros, Î») -> float

Milestone: macros contribute on periodic streams; AMC adjusts Î».

Tests to create with each phase (tiny, deterministic):

tests/test_me_lag.py: learns a 2-lag toy; Acc@1 improves.

tests/test_hazard.py: known Î” distribution â†’ recoverable loglik.

tests/test_resonance.py: synthetic period-p stream â†’ Î _p peak.

tests/test_trust_region.py: enforced KL bound blocks harmful updates.

tests/test_macros.py: spawn from Î _p peak and improve NLL on-cycle.

Focus summary: build the â€œcore-5â€ to run a minimal learning loop; add hazards (waiting), then resonance read-outs, then AMC safety, then macros. Each phase is executable and logged before adding the next knob.


stream-upca/
â”œâ”€â”€ README.md                          # What the project is; quickstart; data format spec
â”œâ”€â”€ pyproject.toml                     # Build/deps; pinned versions
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml                   # Main hyperparams (lags, L, hazard bases, KL Îµ, etc.)
â”‚   â”œâ”€â”€ small.yaml                     # Tiny config for CI/tests
â”‚   â”œâ”€â”€ ablation_roleless.yaml         # Toggle role features off (for comparisons)
â”‚   â””â”€â”€ resonance_only.yaml            # Disable hazards/macros to isolate resonance
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ events.csv                     # seq_id,step,token,namespace,weight,split
â”‚   â””â”€â”€ vocab.csv                      # token,id (optional)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py                # Convert legacy triples â†’ events.csv
â”‚   â”œâ”€â”€ make_toy_sequences.py          # Generate synthetic periodic streams
â”‚   â””â”€â”€ run_experiment.sh              # One-liner wrappers for common runs
â”œâ”€â”€ src/streamupca/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                      # Load/validate YAML; expose dataclasses
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logging.py                 # Structured logs; step-wise metrics emit
â”‚   â”‚   â”œâ”€â”€ math_ops.py                # Safe log-sum-exp, MÃ¶bius Î¼(n), Ramanujan sums
â”‚   â”‚   â”œâ”€â”€ serialization.py           # Checkpoint I/O (state dicts + config + git hash)
â”‚   â”‚   â””â”€â”€ seed.py                    # Reproducible RNG seeding
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ schema.py                  # Typed record for Event(row); validators
â”‚   â”‚   â”œâ”€â”€ dataloaders.py             # Stream iterators; windowing; namespace filters
â”‚   â”‚   â””â”€â”€ streaming_buffer.py        # Ring buffer of recent context; EMA features
â”‚   â”œâ”€â”€ scaffold/
â”‚   â”‚   â”œâ”€â”€ state.py                   # Online state container (A^(â„“), hazards, macros, EMAs)
â”‚   â”‚   â”œâ”€â”€ traces.py                  # Eligibility traces; hazard-survival bookkeeping
â”‚   â”‚   â”œâ”€â”€ trust_region.py            # KL guards; pre/post snapshot & comparison set
â”‚   â”‚   â””â”€â”€ updates.py                 # In-place param updates; normalize/sparsify policies
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ me_lag.py                  # Lag-typed next-token experts A^(â„“); softmax mixer Î²_â„“
â”‚   â”‚   â”œâ”€â”€ hazard_semi_markov.py      # Pairwise hazards h_{ij}(d), survival S, delay pmf f
â”‚   â”‚   â”œâ”€â”€ resonance.py               # W_t build; trace-moments Î¼_k; primitive Î _k via MÃ¶bius
â”‚   â”‚   â”œâ”€â”€ ramanujan.py               # Timeline projections R_q over windows; prime detectors
â”‚   â”‚   â”œâ”€â”€ macros.py                  # Prime-period cyclic ABS (phase HMM); emissions U_m
â”‚   â”‚   â””â”€â”€ mixer.py                   # MEâ€“MA mixture p(x_t|H); Î»_t blending interface
â”‚   â”œâ”€â”€ amc/
â”‚   â”‚   â”œâ”€â”€ controller.py              # Chooses Î»_t; triggers spawn/merge/prune under KL bound
â”‚   â”‚   â”œâ”€â”€ objectives.py              # Surprise, spectral priors, complexity penalties
â”‚   â”‚   â””â”€â”€ structure_ops.py           # Safe structural changes (create macro, adjust bases)
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ metrics_next_token.py      # Accuracy@k, NLL for next-token prediction
â”‚   â”‚   â”œâ”€â”€ metrics_delay.py           # Delay NLL for (iâ†’j,Î”); calibration curves
â”‚   â”‚   â”œâ”€â”€ metrics_resonance.py       # Î _p and R_p tracking; prime-peak persistence
â”‚   â”‚   â””â”€â”€ ablations.py               # Roleless vs. role-typed; hazards on/off; macro on/off
â”‚   â”œâ”€â”€ vis/
â”‚   â”‚   â”œâ”€â”€ plots.py                   # Matplotlib figures for learning curves & spectra
â”‚   â”‚   â””â”€â”€ dashboard.py               # (Optional) lightweight dashboard for live runs
â”‚   â””â”€â”€ runners/
â”‚       â”œâ”€â”€ train_stream.py            # Main loop: read events â†’ update ME/MA â†’ AMC ops â†’ log
â”‚       â”œâ”€â”€ validate.py                # Offline eval on held-out splits
â”‚       â””â”€â”€ simulate_sequences.py      # Simple generators (e.g., obj1,obj1,obj2 motifs)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_hazard.py                 # f_{ij}(Î”) correctness; gradients; corner cases
â”‚   â”œâ”€â”€ test_resonance.py              # Î¼_k/Î _k; Ramanujan R_q on known periodic signals
â”‚   â”œâ”€â”€ test_trust_region.py           # KL guard trips/accepts as intended
â”‚   â”œâ”€â”€ test_macros.py                 # Phase filtering; emission learning; spawn criteria
â”‚   â””â”€â”€ test_end_to_end.py             # Tiny stream sanity: learns a 2/3-prime motif
â””â”€â”€ notebooks/                         # (Optional) exploratory EDA and ablation reports
    â””â”€â”€ 01_periodicity_probe.ipynb



