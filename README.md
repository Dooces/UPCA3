# UPCA3
Its About Time!
A time-first, resonance-driven UPCA (mathematical spec)
0) Observables and timeline
Discrete time tâˆˆNt\in\mathbb{N}tâˆˆN. Streamed tokens/events xtâˆˆVx_t\in\mathcal{V}xtâ€‹âˆˆV (words/objs/evts).
Define binary indicators yt(j)=1[xt=j]y_t(j)=\mathbf{1}[x_t=j]ytâ€‹(j)=1[xtâ€‹=j]. Let a window Wt={tâˆ’W+1,â€¦,t}\mathcal{W}_t=\{t-W+1,\dots,t\}Wtâ€‹={tâˆ’W+1,â€¦,t}.
Delays: for any ordered pair (iâ€‰â£â†’â€‰â£j)(i\!\to\!j)(iâ†’j), let Î”t(iâ†’j)\Delta_t^{(i\to j)}Î”t(iâ†’j)â€‹ be the number of steps between the latest iii before ttt and the next jjj at ttt (well-defined when xt=jx_t=jxtâ€‹=j and some iii occurred earlier).
1) ME (Detail): lag-typed transitions + semi-Markov â€œwaitingâ€
1.1 Lag-typed next-token model
Choose a maximum lag LLL. For each lag â„“âˆˆ{1,â€¦,L}\ell\in\{1,\dots,L\}â„“âˆˆ{1,â€¦,L} maintain a sparse, row-stochastic matrix A(â„“)âˆˆRâ‰¥0âˆ£Vâˆ£Ã—âˆ£Vâˆ£A^{(\ell)}\in\mathbb{R}_{\ge0}^{|\mathcal{V}|\times|\mathcal{V}|}A(â„“)âˆˆRâ‰¥0âˆ£Vâˆ£Ã—âˆ£Vâˆ£â€‹ with rows indexed by past token and columns by next token.
Let Î²â„“â‰¥0\beta_\ell\ge0Î²â„“â€‹â‰¥0 with âˆ‘â„“Î²â„“=1\sum_\ell \beta_\ell=1âˆ‘â„“â€‹Î²â„“â€‹=1. Define logits
zt(j)=âˆ‘â„“=1LÎ²â„“â€‰Axtâˆ’â„“,â€‰j(â„“),pME(xt=jâˆ£Htâˆ’1)=expâ¡zt(j)âˆ‘kâˆˆVexpâ¡zt(k).z_t(j)=\sum_{\ell=1}^L \beta_\ell\, A^{(\ell)}_{x_{t-\ell},\,j}\quad,\qquad p_{\text{ME}}(x_t=j\mid \mathcal{H}_{t-1})=\frac{\exp z_t(j)}{\sum_{k\in\mathcal{V}}\exp z_t(k)}.ztâ€‹(j)=â„“=1âˆ‘Lâ€‹Î²â„“â€‹Axtâˆ’â„“â€‹,j(â„“)â€‹,pMEâ€‹(xtâ€‹=jâˆ£Htâˆ’1â€‹)=âˆ‘kâˆˆVâ€‹expztâ€‹(k)expztâ€‹(j)â€‹.
(Equivalently, view A(â„“)A^{(\ell)}A(â„“) as additive experts over lags.)
1.2 Variable-delay (semi-Markov) waiting for continuations
For each ordered pair (iâ€‰â£â†’â€‰â£j)(i\!\to\!j)(iâ†’j), define a discrete-time hazard hij(d)âˆˆ(0,1)h_{ij}(d)\in(0,1)hijâ€‹(d)âˆˆ(0,1) over delays dâˆˆ{1,2,â€¦â€‰}d\in\{1,2,\dots\}dâˆˆ{1,2,â€¦},
hij(d)=Ïƒâ€‰â£(Î¸ijâŠ¤Ïˆ(d)â€‰+â€‰uiâŠ¤vj),Ïƒ(a)=11+eâˆ’a,h_{ij}(d)=\sigma\!\big(\theta_{ij}^\top \psi(d) \,+\, u_i^\top v_j\big),\quad \sigma(a)=\tfrac{1}{1+e^{-a}},hijâ€‹(d)=Ïƒ(Î¸ijâŠ¤â€‹Ïˆ(d)+uiâŠ¤â€‹vjâ€‹),Ïƒ(a)=1+eâˆ’a1â€‹,
with basis Ïˆ(d)\psi(d)Ïˆ(d) (e.g., [1,logâ¡d,d,dâˆ’1][1,\log d, d, d^{-1}][1,logd,d,dâˆ’1]) and pair embeddings ui,vju_i,v_juiâ€‹,vjâ€‹. The survival and delay pmf are
Sij(Î”)=âˆd=1Î”âˆ’1(1âˆ’hij(d)),fij(Î”)=Sij(Î”)â€‰hij(Î”).S_{ij}(\Delta)=\prod_{d=1}^{\Delta-1}\big(1-h_{ij}(d)\big),\qquad f_{ij}(\Delta)=S_{ij}(\Delta)\,h_{ij}(\Delta).Sijâ€‹(Î”)=d=1âˆÎ”âˆ’1â€‹(1âˆ’hijâ€‹(d)),fijâ€‹(Î”)=Sijâ€‹(Î”)hijâ€‹(Î”).
When xt=jx_t=jxtâ€‹=j follows the most recent iii with delay Î”=Î”t(iâ†’j)\Delta=\Delta_t^{(i\to j)}Î”=Î”t(iâ†’j)â€‹, the time-likelihood term is logâ¡fij(Î”)\log f_{ij}(\Delta)logfijâ€‹(Î”).
1.3 ME instantaneous loss and online gradients
Instantaneous negative log-likelihood (NLL) with regularization:
LtME=âˆ’logâ¡pME(xtâˆ£Htâˆ’1)â€…â€Šâˆ’â€‰â£â€‰â£âˆ‘i:Â last(i)<tâ€‰â£â€‰â£1[xt=j]logâ¡fijâ€‰â£(Î”t(iâ†’j))â€…â€Š+â€…â€ŠÎ»Aâ€‰â£âˆ‘â„“â€‰â£âˆ¥A(â„“)âˆ¥1â€…â€Š+â€…â€ŠÎ»Î¸âˆ¥Î˜âˆ¥22.\mathcal{L}^{\text{ME}}_t = -\log p_{\text{ME}}(x_t\mid \mathcal{H}_{t-1}) \;-\!\!\sum_{i:\ \text{last}(i)<t}\!\!\mathbf{1}[x_t=j]\log f_{ij}\!\big(\Delta_t^{(i\to j)}\big) \;+\;\lambda_A\!\sum_{\ell}\!\|A^{(\ell)}\|_{1} \;+\;\lambda_\theta\|\Theta\|_2^2.LtMEâ€‹=âˆ’logpMEâ€‹(xtâ€‹âˆ£Htâˆ’1â€‹)âˆ’i:Â last(i)<tâˆ‘â€‹1[xtâ€‹=j]logfijâ€‹(Î”t(iâ†’j)â€‹)+Î»Aâ€‹â„“âˆ‘â€‹âˆ¥A(â„“)âˆ¥1â€‹+Î»Î¸â€‹âˆ¥Î˜âˆ¥22â€‹.
For A(â„“)A^{(\ell)}A(â„“) (additive-expert form), the stochastic gradient is
âˆ‚LtMEâˆ‚Ai,j(â„“)=(â€‰pME(jâˆ£Htâˆ’1)âˆ’yt(j)â€‰)â€‰1[xtâˆ’â„“=i]â€…â€Š+â€…â€ŠÎ»Aâ€‰signâ€‰â£(Ai,j(â„“)).\frac{\partial \mathcal{L}^{\text{ME}}_t}{\partial A^{(\ell)}_{i,j}} = \big(\,p_{\text{ME}}(j\mid\mathcal{H}_{t-1})-y_t(j)\,\big)\,\mathbf{1}[x_{t-\ell}=i] \;+\;\lambda_A\,\text{sign}\!\big(A^{(\ell)}_{i,j}\big).âˆ‚Ai,j(â„“)â€‹âˆ‚LtMEâ€‹â€‹=(pMEâ€‹(jâˆ£Htâˆ’1â€‹)âˆ’ytâ€‹(j))1[xtâˆ’â„“â€‹=i]+Î»Aâ€‹sign(Ai,j(â„“)â€‹).
For hazard parameters Î¸ij\theta_{ij}Î¸ijâ€‹,
âˆ‚âˆ‚Î¸ijâ€‰â£(âˆ’logâ¡fij(Î”))=âˆ’(1âˆ’hij(Î”))Ïˆ(Î”)+âˆ‘d=1Î”âˆ’1hij(d)1âˆ’hij(d)â€‰Ïˆ(d).\frac{\partial}{\partial \theta_{ij}} \!\left(-\log f_{ij}(\Delta)\right) = -\Big(1-h_{ij}(\Delta)\Big)\psi(\Delta) +\sum_{d=1}^{\Delta-1}\frac{h_{ij}(d)}{1-h_{ij}(d)}\,\psi(d).âˆ‚Î¸ijâ€‹âˆ‚â€‹(âˆ’logfijâ€‹(Î”))=âˆ’(1âˆ’hijâ€‹(Î”))Ïˆ(Î”)+d=1âˆ‘Î”âˆ’1â€‹1âˆ’hijâ€‹(d)hijâ€‹(d)â€‹Ïˆ(d).
Use eligibility traces eij(t)=Î³eij(tâˆ’1)+1[xt=i]e_{ij}(t)=\gamma e_{ij}(t-1)+\mathbf{1}[x_t=i]eijâ€‹(t)=Î³eijâ€‹(tâˆ’1)+1[xtâ€‹=i] to gate updates only for recently active anchors iii.
2) MA (Abstract): resonance, periods, and cyclic macros
2.1 Autocorrelation and trace-moments on a streaming operator
Build a row-stochastic operator WtW_tWtâ€‹ from the recent A(â„“)A^{(\ell)}A(â„“) (e.g., Wt=âˆ‘â„“â‰¤Lâ€²Î²â„“Î (â„“)W_t=\sum_{\ell\le L'}\beta_\ell \Pi^{(\ell)}Wtâ€‹=âˆ‘â„“â‰¤Lâ€²â€‹Î²â„“â€‹Î (â„“), where Î (â„“)\Pi^{(\ell)}Î (â„“) is the empirical transition at lag â„“\ellâ„“ over Wt\mathcal{W}_tWtâ€‹). Define closed-walk moments
Î¼k(t)=trâ¡(Wtk),Î k(t)=1kâˆ‘dâˆ£kÎ¼(d)â€‰Î¼k/d(t)(MoÂ¨biusÂ inversion),\mu_k(t)=\operatorname{tr}(W_t^k),\qquad \Pi_k(t)=\frac{1}{k}\sum_{d\mid k}\mu(d)\,\mu_{k/d}(t)\quad\text{(MÃ¶bius inversion)},Î¼kâ€‹(t)=tr(Wtkâ€‹),Î kâ€‹(t)=k1â€‹dâˆ£kâˆ‘â€‹Î¼(d)Î¼k/dâ€‹(t)(MoÂ¨biusÂ inversion),
with MÃ¶bius Î¼(â‹…)\mu(\cdot)Î¼(â‹…). A â€œprime toneâ€ at ppp is a spike in Î p(t)\Pi_p(t)Î pâ€‹(t).
2.2 Ramanujan projections on the timeline
Let ata_tatâ€‹ be an activation (e.g., token counts or NLL spike). For modulus qqq,
cq(n)=âˆ‘1â‰¤aâ‰¤qgcdâ¡(a,q)=1e2Ï€ian/q,Rq(Ï„)=1Tâˆ£âˆ‘s=Ï„Ï„+Tâˆ’1asâ€‰cq(s)âˆ£.c_q(n)=\sum_{\substack{1\le a\le q\\ \gcd(a,q)=1}} e^{2\pi i a n/q},\qquad R_q(\tau)=\frac{1}{T}\Big|\sum_{s=\tau}^{\tau+T-1} a_s\, c_q(s)\Big|.cqâ€‹(n)=1â‰¤aâ‰¤qgcd(a,q)=1â€‹âˆ‘â€‹e2Ï€ian/q,Rqâ€‹(Ï„)=T1â€‹â€‹s=Ï„âˆ‘Ï„+Tâˆ’1â€‹asâ€‹cqâ€‹(s)â€‹.
For primes ppp, large Rp(Ï„)R_p(\tau)Rpâ€‹(Ï„) indicates prime-period energy in window [Ï„,Ï„+T)[\tau,\tau+T)[Ï„,Ï„+T).
2.3 Cyclic ABS macros (phase models)
Introduce latent macros m=1,â€¦,Mm=1,\dots,Mm=1,â€¦,M, each with prime period pmp_mpmâ€‹ and phase Ï•m,tâˆˆ{0,â€¦,pmâ€‰â£âˆ’â€‰â£1}\phi_{m,t}\in\{0,\dots,p_m\!-\!1\}Ï•m,tâ€‹âˆˆ{0,â€¦,pmâ€‹âˆ’1} evolving as Ï•m,t+1=(Ï•m,t+1)â€Šmodâ€Špm\phi_{m,t+1}=(\phi_{m,t}+1)\bmod p_mÏ•m,t+1â€‹=(Ï•m,tâ€‹+1)modpmâ€‹. Each macro has phase-conditional emissions UmâˆˆRpmÃ—âˆ£Vâˆ£U_m\in\mathbb{R}^{p_m\times|\mathcal{V}|}Umâ€‹âˆˆRpmâ€‹Ã—âˆ£Vâˆ£:
pm(xt=jâˆ£Ï•m,t)=expâ¡Um[Ï•m,t,j]âˆ‘kexpâ¡Um[Ï•m,t,k].p_m(x_t=j\mid \phi_{m,t})=\frac{\exp U_m[\phi_{m,t},j]}{\sum_k \exp U_m[\phi_{m,t},k]}.pmâ€‹(xtâ€‹=jâˆ£Ï•m,tâ€‹)=âˆ‘kâ€‹expUmâ€‹[Ï•m,tâ€‹,k]expUmâ€‹[Ï•m,tâ€‹,j]â€‹.
Phase filtering (circular HMM) uses
Î±m,t(Ï•)âˆÎ±m,tâˆ’1(Ï•â€‰â£âˆ’â€‰â£1)â€…â€Špm(xtâˆ£Ï•),wm(t)âˆexpâ¡â€‰â£(Îºâ‹…Î pm(t)+Î·â‹…Rpm(Ï„t)).\alpha_{m,t}(\phi)\propto \alpha_{m,t-1}(\phi\!-\!1)\;p_m(x_t\mid \phi),\quad w_m(t)\propto \exp\!\Big(\kappa\cdot \Pi_{p_m}(t)+\eta\cdot R_{p_m}(\tau_t)\Big).Î±m,tâ€‹(Ï•)âˆÎ±m,tâˆ’1â€‹(Ï•âˆ’1)pmâ€‹(xtâ€‹âˆ£Ï•),wmâ€‹(t)âˆexp(Îºâ‹…Î pmâ€‹â€‹(t)+Î·â‹…Rpmâ€‹â€‹(Ï„tâ€‹)).
2.4 MEâ€“MA mixture
p(xtâˆ£Htâˆ’1)=(1âˆ’Î»t)â€‰pME(xtâˆ£Htâˆ’1)â€…â€Š+â€…â€ŠÎ»tâ€‰âˆ‘mÏ€m(t)â€‰pm(xtâˆ£Ï•m,t),p(x_t\mid \mathcal{H}_{t-1}) = (1-\lambda_t)\,p_{\text{ME}}(x_t\mid\mathcal{H}_{t-1}) \;+\;\lambda_t\,\sum_{m} \pi_m(t)\, p_m(x_t\mid \phi_{m,t}),p(xtâ€‹âˆ£Htâˆ’1â€‹)=(1âˆ’Î»tâ€‹)pMEâ€‹(xtâ€‹âˆ£Htâˆ’1â€‹)+Î»tâ€‹mâˆ‘â€‹Ï€mâ€‹(t)pmâ€‹(xtâ€‹âˆ£Ï•m,tâ€‹),
where Ï€m(t)=wm(t)âˆ‘mâ€²wmâ€²(t)\pi_m(t)=\frac{w_m(t)}{\sum_{m'}w_{m'}(t)}Ï€mâ€‹(t)=âˆ‘mâ€²â€‹wmâ€²â€‹(t)wmâ€‹(t)â€‹ and Î»tâˆˆ[0,1]\lambda_t\in[0,1]Î»tâ€‹âˆˆ[0,1] is AMC-controlled (below).
3) AMC: arbitration, consolidation, and trust-region stability
3.1 Surprise, persistence, and off-tone gating
Define instantaneous surprise
Stâ€…â€Š=â€…â€Šâˆ’logâ¡p(xtâˆ£Htâˆ’1)â€…â€Šâˆ’â€‰â£â€‰â£âˆ‘i:last(i)<tâ€‰â£1[xt=j]logâ¡fij(Î”t(iâ†’j)).\mathcal{S}_t \;=\; -\log p(x_t\mid\mathcal{H}_{t-1}) \;-\!\!\sum_{i:\text{last}(i)<t}\!\mathbf{1}[x_t=j]\log f_{ij}(\Delta_t^{(i\to j)}).Stâ€‹=âˆ’logp(xtâ€‹âˆ£Htâˆ’1â€‹)âˆ’i:last(i)<tâˆ‘â€‹1[xtâ€‹=j]logfijâ€‹(Î”t(iâ†’j)â€‹).
Let SË‰t=EMAÏ„(St)\bar{\mathcal{S}}_t=\text{EMA}_\tau(\mathcal{S}_t)SË‰tâ€‹=EMAÏ„â€‹(Stâ€‹) and define off-tone factor for dominant prime pâ‹†p^\starpâ‹† (if present):
gpâ‹†(Î”)={Ï,Î”â€Šmodâ€Špâ‹†âˆˆ{0,1,pâ‹†â€‰â£âˆ’â€‰â£1}Ïƒ,otherwise(Ï>1>Ïƒ>0).g_{p^\star}(\Delta)= \begin{cases} \rho,& \Delta\bmod p^\star\in\{0,1,p^\star\!-\!1\}\\ \sigma,& \text{otherwise} \end{cases}\quad (\rho>1>\sigma>0).gpâ‹†â€‹(Î”)={Ï,Ïƒ,â€‹Î”modpâ‹†âˆˆ{0,1,pâ‹†âˆ’1}otherwiseâ€‹(Ï>1>Ïƒ>0).
Use gpâ‹†g_{p^\star}gpâ‹†â€‹ to scale learning rates for updates induced by delays Î”\DeltaÎ”.
3.2 Arbitration objective and control
AMC chooses Î»t\lambda_tÎ»tâ€‹ and (occasionally) spawns/merges macros by minimizing
LtAMC=SË‰t+Î³câ€‰(#params)âŸcomplexityâˆ’Îºpâ€‰maxâ¡pâˆˆPÎ p(t)âˆ’Îºrâ€‰maxâ¡pâˆˆPRp(Ï„t),\mathcal{L}^{\text{AMC}}_t = \bar{\mathcal{S}}_t + \gamma_c\,\underbrace{\big(\#\text{params}\big)}_{\text{complexity}} - \kappa_p\,\max_{p\in\mathcal{P}}\Pi_p(t) - \kappa_r\,\max_{p\in\mathcal{P}}R_p(\tau_t),LtAMCâ€‹=SË‰tâ€‹+Î³câ€‹complexity(#params)â€‹â€‹âˆ’Îºpâ€‹pâˆˆPmaxâ€‹Î pâ€‹(t)âˆ’Îºrâ€‹pâˆˆPmaxâ€‹Rpâ€‹(Ï„tâ€‹),
subject to a trust-region stability constraint over a guarded query set Qt\mathcal{Q}_tQtâ€‹ (high-support contexts):
1âˆ£Qtâˆ£âˆ‘qâˆˆQtDKLâ€‰â£(Ppre(â‹…âˆ£q)â€‰âˆ¥â€‰Ppost(â‹…âˆ£q))Â â‰¤Â Ïµ.\frac{1}{|\mathcal{Q}_t|}\sum_{q\in\mathcal{Q}_t} D_{\mathrm{KL}}\!\left(P_{\text{pre}}(\cdot\mid q)\,\big\|\,P_{\text{post}}(\cdot\mid q)\right)\ \le\ \epsilon.âˆ£Qtâ€‹âˆ£1â€‹qâˆˆQtâ€‹âˆ‘â€‹DKLâ€‹(Ppreâ€‹(â‹…âˆ£q)â€‹Ppostâ€‹(â‹…âˆ£q))Â â‰¤Â Ïµ.
Any prune/merge/add that violates the bound is rejected or softened. (This formalizes â€œdonâ€™t delete if it tanks success.â€)
3.3 Macro creation/merging criteria
Spawn macro mmm with prime pmp_mpmâ€‹ when both hold on Wt\mathcal{W}_tWtâ€‹:
Î pm(t)â‰¥Ï„Î andRpm(Ï„t)â‰¥Ï„R,\Pi_{p_m}(t) \ge \tau_{\Pi}\quad\text{and}\quad R_{p_m}(\tau_t)\ge \tau_{R},Î pmâ€‹â€‹(t)â‰¥Ï„Î â€‹andRpmâ€‹â€‹(Ï„tâ€‹)â‰¥Ï„Râ€‹,
and the expected risk decreases:
Î”E[SË‰]â€…â€Šâ‰ˆâ€…â€ŠEWtâ€‰â£[âˆ’logâ¡â€‰â£((1âˆ’Î»)pME+Î»pm)]âŸwithÂ macroâ€…â€Šâˆ’â€…â€ŠEWtâ€‰â£[âˆ’logâ¡pME]âŸbaselineâ€…â€Š<â€…â€Šâˆ’Ï„gain,\Delta\mathbb{E}[\bar{\mathcal{S}}]\;\approx\; \underbrace{\mathbb{E}_{\mathcal{W}_t}\!\left[-\log\!\big((1-\lambda)p_{\text{ME}}+\lambda p_m\big)\right]}_{\text{with macro}} \;-\; \underbrace{\mathbb{E}_{\mathcal{W}_t}\!\left[-\log p_{\text{ME}}\right]}_{\text{baseline}} \;<\; -\tau_{\text{gain}},Î”E[SË‰]â‰ˆwithÂ macroEWtâ€‹â€‹[âˆ’log((1âˆ’Î»)pMEâ€‹+Î»pmâ€‹)]â€‹â€‹âˆ’baselineEWtâ€‹â€‹[âˆ’logpMEâ€‹]â€‹â€‹<âˆ’Ï„gainâ€‹,
while satisfying the trust-region constraint.
4) Regularizers that encode â€œresonance-firstâ€
Spectral prior: encourage energy at discovered primes:
Rspec=âˆ’Î±Î âˆ‘pâˆˆPÎ p(t)â€…â€Šâˆ’â€…â€ŠÎ±Râˆ‘pâˆˆPRp(Ï„t).\mathcal{R}_{\text{spec}}=-\alpha_{\Pi}\sum_{p\in\mathcal{P}}\Pi_p(t)\;-\;\alpha_{R}\sum_{p\in\mathcal{P}}R_p(\tau_t).Rspecâ€‹=âˆ’Î±Î â€‹pâˆˆPâˆ‘â€‹Î pâ€‹(t)âˆ’Î±Râ€‹pâˆˆPâˆ‘â€‹Rpâ€‹(Ï„tâ€‹).
Capacity control: Î»Aâˆ¥Aâˆ¥1\lambda_A\|A\|_1Î»Aâ€‹âˆ¥Aâˆ¥1â€‹ (sparseness) and phase-smoothness âˆ‘mâˆ‘Ï•âˆ¥Um[Ï•]âˆ’Um[Ï•â€‰â£âˆ’â€‰â£1]âˆ¥22\sum_m\sum_\phi \|U_m[\phi]-U_m[\phi\!-\!1]\|_2^2âˆ‘mâ€‹âˆ‘Ï•â€‹âˆ¥Umâ€‹[Ï•]âˆ’Umâ€‹[Ï•âˆ’1]âˆ¥22â€‹.
5) Global objective and update sketch
Across time, minimize
minâ¡Î˜Â âˆ‘t(LtMEâ€…â€Š+â€…â€Š(1âˆ’Î»t)â€‰[âˆ’logâ¡pME(xtâˆ£Htâˆ’1)]âŸalreadyÂ inÂ LtMEâ€…â€Š+â€…â€ŠÎ»tâ€‰[âˆ’logâ¡â€‰â£âˆ‘mÏ€m(t)pm(xtâˆ£Ï•m,t)])+Rspec+complexity,\min_{\Theta}\ \sum_{t}\Big(\mathcal{L}^{\text{ME}}_t \;+\; (1-\lambda_t)\,\underbrace{\big[-\log p_{\text{ME}}(x_t\mid\mathcal{H}_{t-1})\big]}_{\text{already in }\mathcal{L}^{\text{ME}}_t} \;+\;\lambda_t\,\big[-\log\!\sum_m \pi_m(t)p_m(x_t\mid\phi_{m,t})\big]\Big) +\mathcal{R}_{\text{spec}} + \text{complexity},Î˜minâ€‹Â tâˆ‘â€‹(LtMEâ€‹+(1âˆ’Î»tâ€‹)alreadyÂ inÂ LtMEâ€‹[âˆ’logpMEâ€‹(xtâ€‹âˆ£Htâˆ’1â€‹)]â€‹â€‹+Î»tâ€‹[âˆ’logmâˆ‘â€‹Ï€mâ€‹(t)pmâ€‹(xtâ€‹âˆ£Ï•m,tâ€‹)])+Rspecâ€‹+complexity,
subject to the AMC trust region at each structural change. Stochastic online updates:
Î˜â†Î˜âˆ’Î·tâ€‰gpâ‹†(Î”)â€‰âˆ‡Î˜(instantaneousÂ loss),\Theta \leftarrow \Theta - \eta_t\,g_{p^\star}(\Delta)\,\nabla_\Theta \big(\text{instantaneous loss}\big),Î˜â†Î˜âˆ’Î·tâ€‹gpâ‹†â€‹(Î”)âˆ‡Î˜â€‹(instantaneousÂ loss),
with EMA-based steps for Î»t,Ï€m(t)\lambda_t,\pi_m(t)Î»tâ€‹,Ï€mâ€‹(t) and periodic structural moves (spawn/merge/prune) only if the KL-bound holds.
6) What this formalization guarantees
Waiting state: encoded by fij(Î”)f_{ij}(\Delta)fijâ€‹(Î”) (semi-Markov); â€œlightningâ†’thunderâ€ is a high-hazard pair at characteristic Î”\DeltaÎ”.
Resonance-first: periods are detected by Î p(t)\Pi_p(t)Î pâ€‹(t) (closed-walk primitives) and RpR_pRpâ€‹ (Ramanujan timeline projection); macros bind them via phases.
Chunk-agnostic: whether input is 1-gram/2-gram/triple, the same lag/hazard machinery applies.
Stability: the KL trust region makes â€œwhat-ifâ€ deletions mathematically safe; spectral priors prevent 2-cycle swamping and favor prime tones.


Unified Scaffold (shared state): holds lag experts 
ğ´
(
â„“
)
A 
(â„“)
 , delay hazards 
â„
ğ‘–
ğ‘—
(
ğ‘‘
)
h 
ij
â€‹
 (d)/
ğ‘“
ğ‘–
ğ‘—
(
Î”
)
f 
ij
â€‹
 (Î”), resonance summaries 
ğœ‡
ğ‘˜
,
Î 
ğ‘
Î¼ 
k
â€‹
 ,Î  
p
â€‹
 , Ramanujan windows 
ğ‘…
ğ‘
R 
p
â€‹
 , the macro library (prime-period, phase HMMs), EMAs, and KL trust-region snapshots. Itâ€™s the only source of truth each module reads/writes.

ME (Detail Engine): fast, token-time updates.
Inputs: recent context/lags; active â€œwaitingâ€ pairs.
Writes: 
ğ´
(
â„“
)
A 
(â„“)
 , hazard params 
ğœƒ
ğ‘–
ğ‘—
,
ğ‘¢
ğ‘–
,
ğ‘£
ğ‘—
Î¸ 
ij
â€‹
 ,u 
i
â€‹
 ,v 
j
â€‹
 .
Outputs: 
ğ‘
ME
(
ğ‘¥
ğ‘¡
â€‰â£
âˆ£
â€‰â£
ğ»
ğ‘¡
âˆ’
1
)
p 
ME
â€‹
 (x 
t
â€‹
 âˆ£H 
tâˆ’1
â€‹
 ), delay pmf 
ğ‘“
ğ‘–
ğ‘—
(
Î”
)
f 
ij
â€‹
 (Î”), uncertainty.
Scope: predicts what/when next; does not create structures.

MA (Abstract Engine): medium-tempo structure discovery.
Inputs: MEâ€™s 
ğ´
(
â„“
)
A 
(â„“)
  (to build 
ğ‘Š
ğ‘¡
W 
t
â€‹
 ), timeline activations.
Writes: 
ğœ‡
ğ‘˜
,
Î 
ğ‘
,
ğ‘…
ğ‘
Î¼ 
k
â€‹
 ,Î  
p
â€‹
 ,R 
p
â€‹
 ; creates/updates cyclic macros (period 
ğ‘
p, phase 
ğœ™
Ï•, emissions 
ğ‘ˆ
U).
Outputs: macro likelihoods 
ğ‘
ğ‘š
(
ğ‘¥
ğ‘¡
â€‰â£
âˆ£
â€‰â£
ğœ™
)
p 
m
â€‹
 (x 
t
â€‹
 âˆ£Ï•), prime evidence.
Scope: detects/resolves rhythms; no arbitration.

AMC (Arbiter): slow, supervisory control with safety.
Inputs: surprise/NLL, MA prime evidence, complexity.
Actions: sets mix 
ğœ†
ğ‘¡
Î» 
t
â€‹
  (MEâ†”MA), scales learning by prime gating 
ğ‘”
ğ‘
g 
p
â€‹
 ; spawns/merges/prunes macros under a KL trust-region; accepts/reverts changes.
Scope: stability, capacity, policy.

Clear interfaces:
MEâ†’MA: provides 
ğ‘Š
ğ‘¡
W 
t
â€‹
  (from 
ğ´
(
â„“
)
A 
(â„“)
 ) for spectra.
MAâ†’AMC: prime/period signals and candidate macros.
AMCâ†’ME/MA: 
ğœ†
ğ‘¡
Î» 
t
â€‹
 , learning-rate gates, and structural decisions.
Scaffold enforces versioning and KL-bounded commits.

Phase 0 â€” Proof-of-Concept â€œcore-5â€ (get a running loop fast)

src/streamupca/config.py

load_config(path) -> Config

validate(Config) -> None

freeze(Config) -> FrozenConfig (immutable view)

src/streamupca/data/dataloaders.py

stream_events(path, *, namespaces=None, split="train") -> Iterator[Event]

window(buffer, W) -> Context (last W tokens + indices)

batcher(iterable, n) -> Iterator[List[Event]] (optional)

src/streamupca/scaffold/state.py

class ScaffoldState:

from_config(cfg) -> ScaffoldState

get_A(lag:int) -> SparseRowMap / set_A(lag, row, col, val)

ema_update(name:str, value:float, tau:float)

checkpoint(save_path) / restore(load_path)

metrics_snapshot() -> Dict[str, float]

src/streamupca/models/me_lag.py (lag-experts only)

class LagExperts:

predict_proba(ctx:Context) -> Probs

nll(event:Event, ctx:Context) -> float

sgd_update(event, ctx, lr:float)

regularize(l1:float)

export_params() / import_params()

src/streamupca/runners/train_stream.py

train(cfg, events_path) â€” main loop: read â†’ predict â†’ loss â†’ update â†’ log

evaluate_next_token(cfg, events_path) -> Dict[str, float]

log_step(step, metrics:Dict)

POC exit criteria: can ingest events.csv, learn unigramâ†’lag patterns, and report Next-Token NLL, Acc@1/5, EMAs. Nothing else.

Phase 1 â€” â€œWaitingâ€ and timing (semi-Markov hazards)
6) src/streamupca/models/hazard_semi_markov.py

class HazardModel:

hazard(i,j,d) -> float

survival(i,j,Î”) -> float

pmf(i,j,Î”) -> float

loglik(i,j,Î”) -> float

update(i,j,Î”, lr) (with eligibility traces)

start_trace(i, t) / end_trace(i, t)

Integrations

Runner: add delay-NLL to loss and logging

State: store u_i, v_j, Î¸_{ij} banks and traces

Phase 2 â€” Resonance probes (no structure changes yet)
8) src/streamupca/models/resonance.py

build_W(state, lags:List[int]) -> SparseOp

trace_moments(W, ks:Iterable[int]) -> Dict[k, Î¼_k]

mobius_invert(mu:Dict[int,float]) -> Dict[k, Î _k]

ramanujan_window(a_t:Sequence[float], q:int, T:int) -> float

update_resonance_cache(state, stats)

src/streamupca/eval/metrics_resonance.py

prime_peaks(Î :Dict) -> List[(p,score)]

timeline_energy(R) -> Dict[p, float]

Milestone: logs show stable Î¼_k/Î _p and Ramanujan energies alongside accuracy.

Phase 3 â€” Minimal AMC (safety only, no macros yet)
10) src/streamupca/scaffold/trust_region.py

snapshot_predictor(state, query_set) -> DistMap

kl_on_queries(pre:DistMap, post:DistMap) -> float

select_query_set(buffer, k:int) -> List[Context]

src/streamupca/amc/controller.py

class AMCController:

choose_lambda(metrics, resonance) -> float

propose_change(state) -> Change (noop initially)

apply_with_kl_guard(state, change, Îµ) -> bool

Milestone: KL guard wired; dry-runs confirm no catastrophic drops.

Phase 4 â€” Cyclic macros (prime-period ABS) and mixing
12) src/streamupca/models/macros.py

class PhaseMacro:

step_phase() / reset_phase()

emission_prob(token) -> float

update_emission(token, lr)

class MacroLibrary:

spawn_from_peak(p:int, seeds:List[token]) -> MacroId

score(token) -> Dict[macro, prob]

merge_or_prune(criteria)

src/streamupca/models/mixer.py

mix_prob(p_me, p_macros, Î») -> Probs

blend_loss(nll_me, nll_macros, Î») -> float

Milestone: macros contribute on periodic streams; AMC adjusts Î».

Tests to create with each phase (tiny, deterministic):

tests/test_me_lag.py: learns a 2-lag toy; Acc@1 improves.

tests/test_hazard.py: known Î” distribution â†’ recoverable loglik.

tests/test_resonance.py: synthetic period-p stream â†’ Î _p peak.

tests/test_trust_region.py: enforced KL bound blocks harmful updates.

tests/test_macros.py: spawn from Î _p peak and improve NLL on-cycle.

Focus summary: build the â€œcore-5â€ to run a minimal learning loop; add hazards (waiting), then resonance read-outs, then AMC safety, then macros. Each phase is executable and logged before adding the next knob.


stream-upca/
â”œâ”€â”€ README.md                          # What the project is; quickstart; data format spec
â”œâ”€â”€ pyproject.toml                     # Build/deps; pinned versions
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml                   # Main hyperparams (lags, L, hazard bases, KL Îµ, etc.)
â”‚   â”œâ”€â”€ small.yaml                     # Tiny config for CI/tests
â”‚   â”œâ”€â”€ ablation_roleless.yaml         # Toggle role features off (for comparisons)
â”‚   â””â”€â”€ resonance_only.yaml            # Disable hazards/macros to isolate resonance
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ events.csv                     # seq_id,step,token,namespace,weight,split
â”‚   â””â”€â”€ vocab.csv                      # token,id (optional)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py                # Convert legacy triples â†’ events.csv
â”‚   â”œâ”€â”€ make_toy_sequences.py          # Generate synthetic periodic streams
â”‚   â””â”€â”€ run_experiment.sh              # One-liner wrappers for common runs
â”œâ”€â”€ src/streamupca/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                      # Load/validate YAML; expose dataclasses
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logging.py                 # Structured logs; step-wise metrics emit
â”‚   â”‚   â”œâ”€â”€ math_ops.py                # Safe log-sum-exp, MÃ¶bius Î¼(n), Ramanujan sums
â”‚   â”‚   â”œâ”€â”€ serialization.py           # Checkpoint I/O (state dicts + config + git hash)
â”‚   â”‚   â””â”€â”€ seed.py                    # Reproducible RNG seeding
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ schema.py                  # Typed record for Event(row); validators
â”‚   â”‚   â”œâ”€â”€ dataloaders.py             # Stream iterators; windowing; namespace filters
â”‚   â”‚   â””â”€â”€ streaming_buffer.py        # Ring buffer of recent context; EMA features
â”‚   â”œâ”€â”€ scaffold/
â”‚   â”‚   â”œâ”€â”€ state.py                   # Online state container (A^(â„“), hazards, macros, EMAs)
â”‚   â”‚   â”œâ”€â”€ traces.py                  # Eligibility traces; hazard-survival bookkeeping
â”‚   â”‚   â”œâ”€â”€ trust_region.py            # KL guards; pre/post snapshot & comparison set
â”‚   â”‚   â””â”€â”€ updates.py                 # In-place param updates; normalize/sparsify policies
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ me_lag.py                  # Lag-typed next-token experts A^(â„“); softmax mixer Î²_â„“
â”‚   â”‚   â”œâ”€â”€ hazard_semi_markov.py      # Pairwise hazards h_{ij}(d), survival S, delay pmf f
â”‚   â”‚   â”œâ”€â”€ resonance.py               # W_t build; trace-moments Î¼_k; primitive Î _k via MÃ¶bius
â”‚   â”‚   â”œâ”€â”€ ramanujan.py               # Timeline projections R_q over windows; prime detectors
â”‚   â”‚   â”œâ”€â”€ macros.py                  # Prime-period cyclic ABS (phase HMM); emissions U_m
â”‚   â”‚   â””â”€â”€ mixer.py                   # MEâ€“MA mixture p(x_t|H); Î»_t blending interface
â”‚   â”œâ”€â”€ amc/
â”‚   â”‚   â”œâ”€â”€ controller.py              # Chooses Î»_t; triggers spawn/merge/prune under KL bound
â”‚   â”‚   â”œâ”€â”€ objectives.py              # Surprise, spectral priors, complexity penalties
â”‚   â”‚   â””â”€â”€ structure_ops.py           # Safe structural changes (create macro, adjust bases)
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ metrics_next_token.py      # Accuracy@k, NLL for next-token prediction
â”‚   â”‚   â”œâ”€â”€ metrics_delay.py           # Delay NLL for (iâ†’j,Î”); calibration curves
â”‚   â”‚   â”œâ”€â”€ metrics_resonance.py       # Î _p and R_p tracking; prime-peak persistence
â”‚   â”‚   â””â”€â”€ ablations.py               # Roleless vs. role-typed; hazards on/off; macro on/off
â”‚   â”œâ”€â”€ vis/
â”‚   â”‚   â”œâ”€â”€ plots.py                   # Matplotlib figures for learning curves & spectra
â”‚   â”‚   â””â”€â”€ dashboard.py               # (Optional) lightweight dashboard for live runs
â”‚   â””â”€â”€ runners/
â”‚       â”œâ”€â”€ train_stream.py            # Main loop: read events â†’ update ME/MA â†’ AMC ops â†’ log
â”‚       â”œâ”€â”€ validate.py                # Offline eval on held-out splits
â”‚       â””â”€â”€ simulate_sequences.py      # Simple generators (e.g., obj1,obj1,obj2 motifs)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_hazard.py                 # f_{ij}(Î”) correctness; gradients; corner cases
â”‚   â”œâ”€â”€ test_resonance.py              # Î¼_k/Î _k; Ramanujan R_q on known periodic signals
â”‚   â”œâ”€â”€ test_trust_region.py           # KL guard trips/accepts as intended
â”‚   â”œâ”€â”€ test_macros.py                 # Phase filtering; emission learning; spawn criteria
â”‚   â””â”€â”€ test_end_to_end.py             # Tiny stream sanity: learns a 2/3-prime motif
â””â”€â”€ notebooks/                         # (Optional) exploratory EDA and ablation reports
    â””â”€â”€ 01_periodicity_probe.ipynb



